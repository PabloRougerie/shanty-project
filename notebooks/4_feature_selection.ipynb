{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3975a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from ml_logic.data_preprocessing import clean_data, resample_pings, vessel_train_test_split\n",
    "from ml_logic.feature_engineering import create_time_series_features\n",
    "from ml_logic.metric import position_extrapolation, haversine_mae\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7188d",
   "metadata": {},
   "source": [
    "# Feature Selection Analysis\n",
    "\n",
    "## Objective\n",
    "\n",
    "After feature engineering showed no improvement, we now test whether **feature selection** can:\n",
    "1. Identify redundant or harmful features\n",
    "2. Improve model performance by removing noise\n",
    "3. Reduce model complexity without losing predictive power\n",
    "\n",
    "**Method**: Permutation importance with cross-validation.\n",
    "\n",
    "**Hypothesis**: Some lag features (especially long-term COG/SOG lags) might be redundant or noisy, and removing them could improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ad00d",
   "metadata": {},
   "source": [
    "We use **480 minutes (8 hours)** as the target horizon because:\n",
    "- This is where ML models start to significantly outperform the baseline (see notebook 3)\n",
    "- It represents a good balance between short-term (where baseline dominates) and long-term predictions\n",
    "- Feature importance patterns are most relevant at this horizon where ML provides value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "954b0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial data frame\n",
    "df = pd.read_parquet(\"../data/raw/AIS_merged_lon-95.0to-77.2_lat22.5to29.5_20241101to20241130.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f571454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablorougerie/code/Projets/shanty_project/ml_logic/data_preprocessing.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[dimension_cols] = df[dimension_cols].replace(0, np.nan)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = clean_data(df) #remove missing values and clean\n",
    "df = resample_pings(df, interval='10min') #uniformize pings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da9cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test, groups_train, groups_val, groups_test = vessel_train_test_split(df, test_size= 0.2, val_size= 0.15, random_state = 273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bb9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction horizon: 480 min. Number of steps: 48\n",
      "Defining lag windows of 80min, 240min, 480min\n",
      "Target prediction horizon: 480 min. Number of steps: 48\n",
      "Defining lag windows of 80min, 240min, 480min\n",
      "Target prediction horizon: 480 min. Number of steps: 48\n",
      "Defining lag windows of 80min, 240min, 480min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#choice of 480min as time horizon : time where ML-approche provide improvement\n",
    "df_train_lag = create_time_series_features(df_train, target_horizon=480, time_step=10,\n",
    "                                     rolling=False, advanced_features=False)\n",
    "\n",
    "df_val_lag = create_time_series_features(df_val, target_horizon=480, time_step=10,\n",
    "                                     rolling=False, advanced_features=False)\n",
    "\n",
    "df_test_lag = create_time_series_features(df_test, target_horizon=480, time_step=10,\n",
    "                                     rolling=False, advanced_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d449e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2754056 samples, 1541 vessels\n",
      "Val: 489972 samples, 274 vessels\n",
      "Test: 821272 samples, 455 vessels\n"
     ]
    }
   ],
   "source": [
    "# Separate X, y and groups for train/val/test sets\n",
    "X_train = df_train_lag.drop(columns=[\"MMSI\", \"BaseDateTime\", \"target_LAT\", \"target_LON\"])\n",
    "y_train = df_train_lag[[\"target_LAT\", \"target_LON\"]]\n",
    "groups_train = df_train_lag[\"MMSI\"]\n",
    "\n",
    "X_val = df_val_lag.drop(columns=[\"MMSI\", \"BaseDateTime\", \"target_LAT\", \"target_LON\"])\n",
    "y_val = df_val_lag[[\"target_LAT\", \"target_LON\"]]\n",
    "groups_val = df_val_lag[\"MMSI\"]\n",
    "\n",
    "X_test = df_test_lag.drop(columns=[\"MMSI\", \"BaseDateTime\", \"target_LAT\", \"target_LON\"])\n",
    "y_test = df_test_lag[[\"target_LAT\", \"target_LON\"]]\n",
    "groups_test = df_test_lag[\"MMSI\"]\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples, {groups_train.nunique()} vessels\")\n",
    "print(f\"Val: {len(X_val)} samples, {groups_val.nunique()} vessels\")\n",
    "print(f\"Test: {len(X_test)} samples, {groups_test.nunique()} vessels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18716cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 748405 samples, 729 vessels\n",
      "Test: 180459 samples, 183 vessels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Train/test split respecting MMSI groups\n",
    "# gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=273)\n",
    "# for train_idx, test_idx in gss.split(X, y, groups):\n",
    "#     X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#     y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "#     #need to isolate a group on the train set for future crossval\n",
    "#     groups_train = groups.iloc[train_idx]\n",
    "#     groups_test = groups.iloc[test_idx]\n",
    "\n",
    "# print(f\"Train: {len(X_train)} samples, {groups_train.nunique()} vessels\")\n",
    "# print(f\"Test: {len(X_test)} samples, {groups_test.nunique()} vessels\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bdfe223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model to use for permutation\n",
    "estimators = {\n",
    "    \"Ridge_scaled\": Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('model', Ridge(alpha=1.0))\n",
    "    ]),\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=273,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "importance_df = pd.DataFrame(index=X_train.columns) #df to stock results of permutation\n",
    "haversine_scorer = make_scorer(haversine_mae, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc470f",
   "metadata": {},
   "source": [
    "## 2. Permutation Importance Analysis\n",
    "\n",
    "### 2.1 LightGBM: Cross-validation + Permutation Importance\n",
    "\n",
    "We use **GroupKFold** (5 folds) to ensure no data leakage: each vessel's data stays together in train or validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44e1445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/5\n",
      "  Fold 2/5\n",
      "  Fold 3/5\n",
      "  Fold 4/5\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LON</td>\n",
       "      <td>570.964678</td>\n",
       "      <td>28.164140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAT</td>\n",
       "      <td>189.160154</td>\n",
       "      <td>4.747449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LON_lag_80min</td>\n",
       "      <td>85.595247</td>\n",
       "      <td>11.280529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LON_lag_480min</td>\n",
       "      <td>69.252351</td>\n",
       "      <td>10.048061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LAT_lag_80min</td>\n",
       "      <td>41.367002</td>\n",
       "      <td>4.488713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LON_lag_240min</td>\n",
       "      <td>30.540199</td>\n",
       "      <td>4.480310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LAT_lag_240min</td>\n",
       "      <td>25.762773</td>\n",
       "      <td>3.555138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LAT_lag_480min</td>\n",
       "      <td>24.036635</td>\n",
       "      <td>2.164564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COG</td>\n",
       "      <td>9.376591</td>\n",
       "      <td>0.282964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOG</td>\n",
       "      <td>5.866807</td>\n",
       "      <td>0.564383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heading</td>\n",
       "      <td>3.765678</td>\n",
       "      <td>0.428612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>COG_lag_240min</td>\n",
       "      <td>2.165879</td>\n",
       "      <td>0.122552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>COG_lag_480min</td>\n",
       "      <td>1.798802</td>\n",
       "      <td>0.236433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SOG_lag_240min</td>\n",
       "      <td>1.779081</td>\n",
       "      <td>0.545130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SOG_lag_480min</td>\n",
       "      <td>1.020040</td>\n",
       "      <td>0.149450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Draft</td>\n",
       "      <td>0.939808</td>\n",
       "      <td>0.213011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Length</td>\n",
       "      <td>0.823044</td>\n",
       "      <td>0.225283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SOG_lag_80min</td>\n",
       "      <td>0.761652</td>\n",
       "      <td>0.182555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>COG_lag_80min</td>\n",
       "      <td>0.647068</td>\n",
       "      <td>0.082540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Width</td>\n",
       "      <td>0.468445</td>\n",
       "      <td>0.113519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Status</td>\n",
       "      <td>0.152823</td>\n",
       "      <td>0.047936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance_mean  importance_std\n",
       "1              LON       570.964678       28.164140\n",
       "0              LAT       189.160154        4.747449\n",
       "12   LON_lag_80min        85.595247       11.280529\n",
       "14  LON_lag_480min        69.252351       10.048061\n",
       "9    LAT_lag_80min        41.367002        4.488713\n",
       "13  LON_lag_240min        30.540199        4.480310\n",
       "10  LAT_lag_240min        25.762773        3.555138\n",
       "11  LAT_lag_480min        24.036635        2.164564\n",
       "3              COG         9.376591        0.282964\n",
       "2              SOG         5.866807        0.564383\n",
       "4          Heading         3.765678        0.428612\n",
       "19  COG_lag_240min         2.165879        0.122552\n",
       "20  COG_lag_480min         1.798802        0.236433\n",
       "16  SOG_lag_240min         1.779081        0.545130\n",
       "17  SOG_lag_480min         1.020040        0.149450\n",
       "8            Draft         0.939808        0.213011\n",
       "6           Length         0.823044        0.225283\n",
       "15   SOG_lag_80min         0.761652        0.182555\n",
       "18   COG_lag_80min         0.647068        0.082540\n",
       "7            Width         0.468445        0.113519\n",
       "5           Status         0.152823        0.047936"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_importance = [] #list to stock scores in each fold\n",
    "gkf = GroupKFold(n_splits= 5) #for crossval respecting MMSI\n",
    "\n",
    "#generating folds without cutting through a single boat ping sequence\n",
    "for fold_idx, (train_fold_idx, val_fold_idx) in enumerate(gkf.split(X_train, y_train, groups= groups_train)):\n",
    "    print(f\"  Fold {fold_idx + 1}/5\")\n",
    "    #generating the train and val sets\n",
    "    X_train_fold = X_train.iloc[train_fold_idx]\n",
    "    X_val_fold = X_train.iloc[val_fold_idx]\n",
    "    y_train_fold = y_train.iloc[train_fold_idx]\n",
    "    y_val_fold = y_train.iloc[val_fold_idx]\n",
    "\n",
    "\n",
    "    model_fold = MultiOutputRegressor(estimators[\"LightGBM\"])\n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Permutation importance on validation fold\n",
    "    importance = permutation_importance(\n",
    "        model_fold, X_val_fold, y_val_fold,\n",
    "        n_repeats=6,\n",
    "        random_state=273,\n",
    "        scoring=haversine_scorer,\n",
    "        n_jobs= -1)\n",
    "\n",
    "    #list of lists (each list contains the means of the 6 MAE scores of permutation, for each featutre)\n",
    "    fold_importance.append(importance.importances_mean)\n",
    "\n",
    "fold_importance = np.array(fold_importance) #conversion in 2D array shape (5, n_features)\n",
    "\n",
    "importance_df = pd.DataFrame({\"feature\": X_train.columns,\n",
    "                              \"importance_mean\": fold_importance.mean(axis= 0),\n",
    "                              \"importance_std\": fold_importance.std(axis= 0)\n",
    "                              }).sort_values(by=\"importance_mean\",  ascending= False)\n",
    "\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315d5ba",
   "metadata": {},
   "source": [
    "### Interpretation: LightGBM Permutation Importance\n",
    "\n",
    "**Most important features**:\n",
    "- **Position features (LAT, LON) and their lags dominate importance** - these are the most predictive features for trajectory prediction\n",
    "- **Vessel dimensions (Length, Width, Draft) have moderate importance** - provide useful context about vessel behavior and movement patterns\n",
    "\n",
    "**Least important features** (candidates for removal):\n",
    "- **`COG_lag_480min`**: \n",
    "- **`COG_lag_240min`**: \n",
    "- **`SOG_lag_240min`**: \n",
    "- **`Heading`**: \n",
    "- **`COG_lag_80min`**: \n",
    "\n",
    "**Why these features are less important?**\n",
    "- Long-term COG lags (240min, 480min) are less predictive than position lags at the same horizons\n",
    "- SOG at 240min may be redundant with other SOG lags that are more informative\n",
    "- Heading and short-term COG lags show minimal individual contribution compared to position features\n",
    "\n",
    "**Important caveat**: Low **individual** importance does not necessarily mean these features are useless. In tree-based models like LightGBM, features can contribute through **interactions** with other features, even if they appear unimportant in isolation. We will validate this via cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9fc79",
   "metadata": {},
   "source": [
    "### 2.2 Ridge: Cross-validation + Permutation Importance\n",
    "\n",
    "Same methodology as LightGBM to compare feature importance patterns between linear and tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d82f9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1/5\n",
      "  Fold 2/5\n",
      "  Fold 3/5\n",
      "  Fold 4/5\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LON</td>\n",
       "      <td>1835.712268</td>\n",
       "      <td>80.316385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAT</td>\n",
       "      <td>838.816096</td>\n",
       "      <td>57.714071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LON_lag_80min</td>\n",
       "      <td>827.975700</td>\n",
       "      <td>64.717416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LAT_lag_80min</td>\n",
       "      <td>439.476716</td>\n",
       "      <td>62.476026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LON_lag_240min</td>\n",
       "      <td>257.625960</td>\n",
       "      <td>10.265817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LAT_lag_240min</td>\n",
       "      <td>160.181284</td>\n",
       "      <td>9.458565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LON_lag_480min</td>\n",
       "      <td>158.222069</td>\n",
       "      <td>8.383689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LAT_lag_480min</td>\n",
       "      <td>1.967266</td>\n",
       "      <td>1.187352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SOG_lag_80min</td>\n",
       "      <td>0.569919</td>\n",
       "      <td>0.121975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COG</td>\n",
       "      <td>0.434924</td>\n",
       "      <td>0.044662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SOG_lag_480min</td>\n",
       "      <td>0.386383</td>\n",
       "      <td>0.054671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Draft</td>\n",
       "      <td>0.177286</td>\n",
       "      <td>0.063637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>COG_lag_240min</td>\n",
       "      <td>0.145571</td>\n",
       "      <td>0.021854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Width</td>\n",
       "      <td>0.132646</td>\n",
       "      <td>0.026497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOG</td>\n",
       "      <td>0.118311</td>\n",
       "      <td>0.106320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Length</td>\n",
       "      <td>0.105460</td>\n",
       "      <td>0.026005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Status</td>\n",
       "      <td>0.098718</td>\n",
       "      <td>0.039561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>COG_lag_80min</td>\n",
       "      <td>0.095727</td>\n",
       "      <td>0.017029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SOG_lag_240min</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.025209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>COG_lag_480min</td>\n",
       "      <td>0.010316</td>\n",
       "      <td>0.007736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heading</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>0.013134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance_mean  importance_std\n",
       "1              LON      1835.712268       80.316385\n",
       "0              LAT       838.816096       57.714071\n",
       "12   LON_lag_80min       827.975700       64.717416\n",
       "9    LAT_lag_80min       439.476716       62.476026\n",
       "13  LON_lag_240min       257.625960       10.265817\n",
       "10  LAT_lag_240min       160.181284        9.458565\n",
       "14  LON_lag_480min       158.222069        8.383689\n",
       "11  LAT_lag_480min         1.967266        1.187352\n",
       "15   SOG_lag_80min         0.569919        0.121975\n",
       "3              COG         0.434924        0.044662\n",
       "17  SOG_lag_480min         0.386383        0.054671\n",
       "8            Draft         0.177286        0.063637\n",
       "19  COG_lag_240min         0.145571        0.021854\n",
       "7            Width         0.132646        0.026497\n",
       "2              SOG         0.118311        0.106320\n",
       "6           Length         0.105460        0.026005\n",
       "5           Status         0.098718        0.039561\n",
       "18   COG_lag_80min         0.095727        0.017029\n",
       "16  SOG_lag_240min         0.036842        0.025209\n",
       "20  COG_lag_480min         0.010316        0.007736\n",
       "4          Heading        -0.008178        0.013134"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_importance = []\n",
    "gkf = GroupKFold(n_splits= 5)\n",
    "for fold_idx, (train_fold_idx, val_fold_idx) in enumerate(gkf.split(X_train, y_train, groups= groups_train)):\n",
    "    print(f\"  Fold {fold_idx + 1}/5\")\n",
    "    X_train_fold = X_train.iloc[train_fold_idx]\n",
    "    X_val_fold = X_train.iloc[val_fold_idx]\n",
    "    y_train_fold = y_train.iloc[train_fold_idx]\n",
    "    y_val_fold = y_train.iloc[val_fold_idx]\n",
    "\n",
    "\n",
    "    model_fold = MultiOutputRegressor(estimators[\"Ridge_scaled\"])\n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Permutation importance on validation fold\n",
    "    importance = permutation_importance(\n",
    "        model_fold, X_val_fold, y_val_fold,\n",
    "        n_repeats=6,\n",
    "        random_state=273,\n",
    "        scoring=haversine_scorer,\n",
    "        n_jobs= -1)\n",
    "\n",
    "    fold_importance.append(importance.importances_mean)\n",
    "\n",
    "fold_importance = np.array(fold_importance)\n",
    "importance_df_ridge = pd.DataFrame({\"feature\": X_train.columns,\n",
    "                              \"importance_mean\": fold_importance.mean(axis= 0),\n",
    "                              \"importance_std\": fold_importance.std(axis= 0)\n",
    "                              }).sort_values(by=\"importance_mean\",  ascending= False)\n",
    "\n",
    "importance_df_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6d4ca",
   "metadata": {},
   "source": [
    "### Interpretation: Ridge Permutation Importance\n",
    "\n",
    "**Most important features**:\n",
    "- **Position features (LAT, LON) and their lags** - similar to LightGBM, these dominate importance\n",
    "- **Vessel dimensions** - moderate importance, consistent with LightGBM\n",
    "\n",
    "**Least important features** (candidates for removal):\n",
    "- **`COG_lag_480min`**: Very low importance - consistent with LightGBM findings\n",
    "- **`COG_lag_240min`**: Low importance - same pattern as LightGBM\n",
    "- **`SOG_lag_240min`**: Low importance - redundant with other SOG features\n",
    "- **`Heading`**: Low importance - minimal contribution\n",
    "- **`COG_lag_80min`**: Low importance - consistent with LightGBM\n",
    "\n",
    "**Comparison with LightGBM**:\n",
    "- Ridge shows **similar patterns** but with different magnitudes (linear model vs tree-based)\n",
    "- Both models agree on which features are **least important individually**\n",
    "- **Consensus low-importance features**: `COG_lag_240min`, `COG_lag_480min`, `SOG_lag_240min`, `Heading`, `COG_lag_80min`\n",
    "\n",
    "**Key difference**:\n",
    "- **Linear models (Ridge)** cannot capture feature interactions, so low individual importance is more meaningful\n",
    "- **Tree-based models (LightGBM)** can capture interactions, so features with low individual importance might still contribute through combinations with other features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951c1b3",
   "metadata": {},
   "source": [
    "### Decision: Features to Remove\n",
    "\n",
    "**Selected features for removal**:\n",
    "1. **`COG_lag_480min`** - Very low importance in both models\n",
    "2. **`COG_lag_240min`** - Low importance in both models\n",
    "3. **`SOG_lag_240min`** - Low importance, redundant with other SOG lags\n",
    "4. **`Heading`** - Low importance, minimal contribution\n",
    "5. **`COG_lag_80min`** - Low/negative importance in both models\n",
    "\n",
    "**Rationale**:\n",
    "- **Consensus between models**: Both LightGBM and Ridge agree these features have low individual importance\n",
    "- **Long-term lags less predictive**: COG lags at 240min and 480min are less useful than position lags at the same horizons\n",
    "- **Model simplicity**: Removing low-importance features reduces complexity\n",
    "\n",
    "**Validation approach**:\n",
    "We will test the impact via cross-validation to determine:\n",
    "- If removal improves performance \n",
    "- If these features contribute through interactions in tree-based models (LightGBM)\n",
    "- If the trade-off between simplicity and performance is acceptable\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feature Selection Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b694976",
   "metadata": {},
   "source": [
    "#### Test with selected features with LGBM model (crossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3503a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 5 features: ['COG_lag_480min', 'SOG_lag_240min', 'COG_lag_240min', 'Heading', 'COG_lag_80min']\n"
     ]
    }
   ],
   "source": [
    "# Removing features with negative or very low importance on both models\n",
    "features_to_remove = [\"COG_lag_480min\",\n",
    "\"SOG_lag_240min\",\n",
    "\"COG_lag_240min\",\n",
    "\"Heading\",\n",
    "\"COG_lag_80min\"]\n",
    "features_to_keep = [f for f in X_train.columns if f not in features_to_remove]\n",
    "\n",
    "print(f\"Removing {len(features_to_remove)} features: {features_to_remove}\")\n",
    "\n",
    "# Create datasets with selected features\n",
    "X_train_selected = X_train[features_to_keep]\n",
    "X_test_selected = X_test[features_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f27f79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "  With all features\n",
      "  With selected features...\n",
      "\n",
      "  MAE with all features: 22.841 ± 0.614 km\n",
      "  MAE with selected features: 23.109 ± 0.840 km\n",
      "  Improvement: -1.17%\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation comparison: all features vs selected features\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Define LightGBM parameters (reusable for CV and final model)\n",
    "lgbm_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 273,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Test LightGBM\n",
    "print(\"LightGBM\")\n",
    "\n",
    "# With all features (separate instance for CV)\n",
    "print(\"  With all features\")\n",
    "lgbm_cv_all = LGBMRegressor(**lgbm_params)\n",
    "scores_all = cross_val_score(\n",
    "    MultiOutputRegressor(lgbm_cv_all),\n",
    "    X_train, y_train,\n",
    "    cv=gkf,\n",
    "    groups=groups_train,\n",
    "    scoring=haversine_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# With selected features (separate instance for CV)\n",
    "print(\"  With selected features...\")\n",
    "lgbm_cv_selected = LGBMRegressor(**lgbm_params)\n",
    "scores_selected = cross_val_score(\n",
    "    MultiOutputRegressor(lgbm_cv_selected),\n",
    "    X_train_selected, y_train,\n",
    "    cv=gkf,\n",
    "    groups=groups_train,\n",
    "    scoring=haversine_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "mae_all = -scores_all.mean() #negative bc sklearn negativates the metric\n",
    "mae_selected = -scores_selected.mean()\n",
    "improvement = ((mae_all - mae_selected) / mae_all) * 100\n",
    "\n",
    "print(f\"\\n  MAE with all features: {mae_all:.3f} ± {scores_all.std():.3f} km\")\n",
    "print(f\"  MAE with selected features: {mae_selected:.3f} ± {scores_selected.std():.3f} km\")\n",
    "print(f\"  Improvement: {improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9004f5",
   "metadata": {},
   "source": [
    "#### Test with selected features on Ridge model (crossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e38109fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge (scaled):\n",
      "  With all features...\n",
      "  With selected features...\n",
      "\n",
      "  MAE with all features: 27.135 ± 0.415 km\n",
      "  MAE with selected features: 27.106 ± 0.434 km\n",
      "  Improvement: 0.11%\n"
     ]
    }
   ],
   "source": [
    "# Test Ridge with scaling\n",
    "print(\"\\nRidge (scaled):\")\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# With all features\n",
    "print(\"  With all features...\")\n",
    "scores_ridge_all = cross_val_score(\n",
    "    MultiOutputRegressor(ridge_pipeline),\n",
    "    X_train, y_train,\n",
    "    cv=gkf,\n",
    "    groups=groups_train,\n",
    "    scoring=haversine_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# With selected features\n",
    "print(\"  With selected features...\")\n",
    "scores_ridge_selected = cross_val_score(\n",
    "    MultiOutputRegressor(ridge_pipeline),\n",
    "    X_train_selected, y_train,\n",
    "    cv=gkf,\n",
    "    groups=groups_train,\n",
    "    scoring=haversine_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "mae_ridge_all = -scores_ridge_all.mean()\n",
    "mae_ridge_selected = -scores_ridge_selected.mean()\n",
    "improvement_ridge = ((mae_ridge_all - mae_ridge_selected) / mae_ridge_all) * 100\n",
    "\n",
    "print(f\"\\n  MAE with all features: {mae_ridge_all:.3f} ± {scores_ridge_all.std():.3f} km\")\n",
    "print(f\"  MAE with selected features: {mae_ridge_selected:.3f} ± {scores_ridge_selected.std():.3f} km\")\n",
    "print(f\"  Improvement: {improvement_ridge:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb062c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overall Feature Selection Conclusion\n",
    "\n",
    "**Key Findings**:\n",
    "- **Feature selection does not improve performance** for LightGBM (slight deterioration of -1.17%)\n",
    "- **Minimal benefit for Ridge** (+0.11% improvement)\n",
    "- The base feature set was already well-designed - removing features reduces available information\n",
    "- Tree-based models (LightGBM) benefit from feature interactions, making even low-importance features potentially useful\n",
    "\n",
    "**Decision**:\n",
    "Despite the small performance trade-off for LightGBM, we proceed with **selected features** for:\n",
    "1. **Model simplicity**: Fewer features reduce complexity and training time\n",
    "\n",
    "\n",
    "4. **Marginal impact**: The 1.17% deterioration is small compared to potential benefits of a cleaner feature set\n",
    "\n",
    "**Removed features**: `COG_lag_480min`, `SOG_lag_240min`, `COG_lag_240min`, `Heading`, `COG_lag_80min`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "***Proceed to hyperparameter tuning (notebook 5) with the selected feature set to further optimize model performance.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20b7ec",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vessel_tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
